# Learning from rewards and observations - improved DQN and policy search (with function approximation)

### Pre
- [ ] AIMA 21 (focus on policy search
- [ ] [[Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf|Rainbow paper]]
### Post

### Outline

- 00:03:00 - **neural networks**
	- 00:06:42 - practical implementation considerations 
		- 00:10:20 - data augmentation intuition and motivation
		- 00:12:40 - model considerations
		- 00:16:49 - learning optimisation
		- 00:21:33 - evaluation criteria
	- 00:33:24 - RL with NN
		- 00:37:53 - update rule walkthrough
		- 00:41:05 - tricks used in the basic DQN algorithm ; robust/Huber loss ; replay buffer
	- 00:42:25 - [[Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf|rainbow paper]] (not required knowledge) :  improvements to the algo since the Atari paper
		- prioritized replay suggested for coursework improvement
- **part 2 - policy search methods (overview, but unlikely to be examinable)**
- 00:00:14 - Q&A coursework
- 00:13:28 - intuition/overview
	- advantage : modeling policy function simpler than the Q action-value function
- 00:16:50 - reinforcement steps
- 00:25:53 - derivation not required ; key issues
- 00:29:46 - algo improvement : reinforcement with baseline
- 00:32:02 - **part 3 - actor-critic method**
- 00:35:51 - properties
- 00:36:36 - comment about what's examinable ; essentially only what's going on in the algorithm 

### Summary

### Key Insights (questions form)

### Relevant Exam Questions