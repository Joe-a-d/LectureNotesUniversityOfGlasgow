\section{Efficient Sorting}

\key{recursion}{divide-and-conquer}{merge}{Merge-sort}{Quick-Sort}{Stable
Solution}{recurrence equations}

\defn{divide-and-conquer}{an algorithm design paradigm based on multi-branched
		recursion; the problem is recursively broken down into simpler problems
		until one is simple enough to be solved directly. The solutions to each
sub-problem are then merged back together}

\subsection{Merge Algorithm}

		\defn{Sentinel}{a number which is much larger than any other number in the
		array. It can be represented by \texttt{Integer.MAX\_VALUE}}

		\rem{A more general definition sees sentinel as any value which is interpreted
		as a condition to terminate the algorithm}

		\defn{Stable}{an algorithm which preserves the input order of repeated elements}

		\rem{note that this is only relevant if the repeated elements are somehow
		distinguishable (e.g deck of cards , (rank,value))}

		\defn{Key}{for data being recorded as a tuple of values, the key is the value
		used to sort (e.g (name, surname) , K=S)}

		\defn{Merging Algorithm}{an algorithm which takes two sorted sequences and
		returns a single combined sequence~\cite{merge}}

\subsubsection{Overview}

		\par{The merge algorithm takes an unsorted array and 3 indices, it then splits
		the array into two subarrays and copies each subarray into memory. Finally, it
		iterates over every element of the original array, for each iteration it
		compares the current elements of the 2 subarrays against each other, replacing
		the appropriate value into the original array and increasing the index of the
		subarray by $1$}

\subsubsection{Formal Definition}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined\KwData{Array $A$ , indices $p,q,r$}
	\SetKwFunction{KwFn}{Merge}
	\KwResult{sorted subarray $A[p..r]$}
	\KwFn{A,p,q,r}\\
	$n_1$ := q - p + 1{\tcc*[r]{initialize subarrays' size}}\;
	$n_2$ := r - q\;
	L[0..$n_1$] := A[p..q]{\tcc*[r]{copy to subarrays}}\;
	R[$n_2$..r] := A[q+1..r]\;
	L[$n_1$] := $\infty${\tcc*[r]{add the sentinel values to end of
subarrays}}\;
	R[$n_2$] := $\infty$\;
	i,j := 0\;
	\For{k=p \KwTo r}{\;
	    \uIf{L[i] $\leq$ R[j]}{\;
			A[k] := L[i]\;
			i := i + 1\;
		}
		\uElse{\;
			A[k] := R[j]\;
			j := j + 1\;
		}
	}
		\caption{Merge}
\end{algorithm}

\subsubsection{Properties}

		\begin{itemize}
			\item\textbf{Running Time : } $O(n)$ , since the initialization of
the $L , R$ subarrays takes $O(n)$ and the loop is executed $n$ times and
contains only constant-time operations
			\item\textbf{Stable}
			\item\textbf{Memory Requirements : } $O(n)$ to store $L, R$	
		\end{itemize}

\subsection{Merge-Sort}

\par{The main advantage of this algorithm is that it takes $N \log N$ to sort an array with $N$ elements. It's main downside is that it uses extra space proportional to $N$}

\rem{See informal definition in previous section}

\subsubsection{Formal Definition}

	\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined\KwData{Array $A$ , indices $p,r$}
	\SetKwFunction{KwFn}{Merge-Sort}
	\SetKwFunction{KwFnn}{Merge}
	\KwResult{sorted array $A[p..r]$}
	\KwFn{A,p,r}\\
	\uIf{p < r}{
		q := (p + r) / 2\;
		\KwFn(A,p,q)\;
		\KwFn{A,q+1,r}\;
		\KwFnn{A,p,q,r}\;
	}
	\caption{Merge-Sort Algorithm}
	\end{algorithm}

\subsubsection{Execution \& Properties}

\rem{to sort an array $A$ with $n$ elements, we would call
\texttt{MERGE-SORT(A,0,n-1)}}

\par{Note how the algorithm gives rise to a binary recursive tree, since we
split each subarray into 2 with each iteration until we hit the stopping
condition (single element array). So, the first subarray is entirely sorted,
before the second \texttt{MERGE-SORT} call. The two \ita{half-subarrays} returned
from each call are then merged by calling \texttt{MERGE}. The process is then
repeated in the right branch}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{mergeTree}
\end{figure}

\begin{itemize}
	\item\textbf{Stable}
	\item\textbf{Memory} $O(n)$
	\item\textbf{Running Time} $O(n \log n)$
\end{itemize}

\subsubsection{Improvements}

	\begin{itemize}
			\item[]\textbf{In-place Merge}
			\item[]\textbf{Bottom-up Iterative}
			\item[]\textbf{\texttt{IS} for small $n$}
	\end{itemize}
\subsection{Recurrence Equations}


\defn{Recurrence Equation}{describe the overall running time of a problem of
size $n$ in terms of the running time on smaller inputs}
	

\par{Recursive algorithms can be described via \ita{recurrence equations}. Take
$t(n)$ to denote the \ita{worst-case} running time of \texttt{MERGE-SORT(n)},
then we can characterize $t(n)$ by means of an equation where $t(n)$ is
recursively expressed in terms of itself.
}
\par{If \texttt{MERGE-SORT(n)} has running time $T(n)$ then, each recursive call
will \texttt{MERGE-SORT(n/2)} will run on $T(n/2)$ time. Hence, we can define
$T(n)$ recursively as follows}

$$ T(n) = \left\{ 
	\begin{array}{ll}
		b & n \leq 1 \quad \text{(base case)} \\
		2T(n/2) + cn 
	\end{array}
\right.
$$
\subsubsection{Iterative Method}
\par{Though correct, a more informative definition will be one that does not
involve $T(n)$ itself. From its \ita{closed-form} characterization, we can
define it in \ita{big-Oh} terms. We do this, by iterative substitution on the
RHS of the recurrence relation until the base case is reached}
\par{Continuing with the expression above, if we substitute $n$ by $n/2$ in the RHS we
essentially double the children arrays, and we get}

\begin{align*}
	T(n) &= 2(2T((n/2)/2) + c(n/2)) + cn \\
		&= 2^2t(n/2^2) + 2(cn/2) + cn \\
		&= 2^2t(n/2^2) + 2cn
\end{align*}

\par{If we repeat the process, assuming that $n$ is relatively large, then we
find the general pattern}

$$t(n) = 2^it(n/2^i) +  icn$$

\par{In order to determine the base case, we look at the original definition,
and observe that  we need to find $t(n)$ , for $n \leq 1$. Given our general
argument $\frac{n}{2^i}$ we need $2^i = n \iff i = \log n$. Hence, substituting
$i$ by $ \log n$ gives}

\begin{align*}
	t(n) &= 2^{\log n} t(n/2^{\log n}) + cn\log n \\
		&= nt(1) + cn\log n \\
		&= nb + cn\log n
\end{align*}

\par{Lastly, since $b,c$ are constants, we have shown that $t(n)$ is $O(n\log
n)$}

\subsubsection{Master Method}

	\par{There is a general form of recurrence relation that arises in the
analysis of the divide-and-conquer algorithms}

$$T(n) = aT(n/b) + f(n) \qquad a , b \geq 1$$

	\par{Then, for $f(n) = \Theta(n^c)$ , the solution will be one of the
following 3 cases}

	\begin{enumerate}
		\item $
\mathrm{c}<\log _{\mathrm{b}} \mathrm{a} \text { then } \mathrm{T}(\mathrm{n})=\Theta\left(n^{\log _{b} a}\right)
$
		\item $
\mathrm{c}=\log _{\mathrm{b}} \mathrm{a} \text { then } \mathrm{T}(\mathrm{n})=\Theta(\mathrm{n} \leq \log \mathrm{n})
$
		\item $
\mathrm{c}>\log _{\mathrm{b} 2} \mathrm{a} \text { then } \mathrm{T}(\mathrm{n})=\Theta(\mathrm{f}(\mathrm{n}))
$
	\end{enumerate}

\par{For \texttt{MERGE-SORT}, we have $a=2 , b=2 , f(n) = \Theta(n) , c = 1$ .
Hence, the solution is given by case $2$ , $\Theta(n^c \log n) = \Theta(n \log
n)$}

\subsubsection{Tree Method}

\rem{See lecture 5 slides 52-60}


\subsection{Quick-Sort}

\subsubsection{Overview}

	\par{The \texttt{Quick-Sort} algorithm is also of the
			\ita{divide-and-conquer} type, the main difference from the previous
			one being that the bulk of the work happens in the \ita{divide}
	stage}

	\rem{The following partition method \ita{Lomuto} is not as efficient as
	\ita{Hoares'}}

	\begin{itemize}
			\item\textbf{Divide : }\par{ given an array $A$ and an index $q$
							we partition $A$ into $A[lo..q-1]$ to $A[q+1..hi]$
							with the former containing all elements smaller than
							$A[q]$ and the latter all the ones which are greater
					than $A[q]$}
			\item\textbf{Conquer :}\par{sort by recurring ; the initial call
							will branch out on one of the main subarrays and the
					second on the other}

	\end{itemize}

	\rem{No merge stage required, since each subarray will be sorted
	\ita{in-place}}

\subsubsection{Informal Definition}

\begin{itemize}
		\item\textbf{Partition}
				\begin{enumerate}\mymarginpar{$lo , hi$ are passed on as
								arguments and represent the first and last index
						of $A$}
						\item set $i$ index to $lo$ , which will keep track of
								the \ita{less-than} subarray
						\item set the pivot to $A[hi]$
						\item set $j$ index to $lo$, which will keep track of
								the \ita{greater-equal} subarray
						\item scan the array from $j$ to $hi-1$
						\begin{enumerate}
								\item compare the current element $A[j]$ with
										the pivot $p$
										\begin{itemize}
												\item $A[j] < p$ : \texttt{i++}
														;$ A[i]
														\leftrightarrow A[j]$ 
												\item $A[j] \geq p$ : keep
														scanning (i.e
														\texttt{j++}) 
										\end{itemize}
						\end{enumerate}
						\item in order to create the $L, R$ partitions
								we swap $p$ with $A[i]$ (i.e the first
								element of $R$)
						\item Lastly, we return $i$, the pivot index in order to
								be used as $hi, lo$ index for the recurrence
								calls
				\end{enumerate}
		\item\textbf{Recur}
					\begin{enumerate}
							\item check that the base case has not been reached
									$lo < hi$
							\item set $p$ to be the return value of
									\texttt{PARTITION}
							\item call \texttt{quick-sort} on $L$, i.e. call it on
									$A[lo..p-1]$
							\item call \texttt{quick-sort} on $R$, i.e call it on
									$A[p+1..hi]$
					\end{enumerate}

	\end{itemize}

	\rem{Note that on each call we exclude $p$ from the subarrays, since it is
	already sorted. This is why there is no need for merging}
								
\subsubsection{Formal Definition}

	\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined\KwData{Array $A$ , indices $lo , hi$}
	\SetKwFunction{KwFn}{PARTITION}
	\SetKwFunction{KwFnn}{SWAP}
	\KwResult{pivot index $i$}
	\KwFn{A,$lo, hi$}\\
	pivot := A[hi]\;
	$i$ := $lo$\;
	\For{$j=lo$ \KwTo $hi$}{
			\uIf{A[j] $<$  pivot}{
					\KwFnn{A[j], A[i]}\;
					i++\;
			}
			\KwFnn{A[i] , A[hi]}
	}
	\KwRet{$i$}
	\caption{PARTITION}
	\end{algorithm}

	\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined\KwData{Array $A$, indices $lo, hi$}
	\SetKwFunction{KwFn}{QUICKSORT}
	\SetKwFunction{KwFnn}{PARTITION}
	\KwResult{sorted array $A[lo..hi]$}
	\KwFn{A,$lo, hi$}\;
	\uIf{$lo < hi$}{
			p := \KwFnn{A , $lo, hi$}\;
			\KwFn{A , $lo$ , $p-1$}\;
			\KwFn{A , $p+1$, $hi$}\;
	}
	\caption{Quick-Sort}
	\end{algorithm}
	
	\subsubsection{Properties}

	\begin{itemize}
			\item\textbf{Not Stable}
			\item\textbf{Memory} $0$ ;  \ita{in-place}
			\item\textbf{Running-Time} cost of partitioning is $O(n)$
					\begin{itemize}
							\item[]\ita{Best} the pivot is in the middle and so
									each subarray is exactly $n/2$ . Hence, $O(n
									\log n$
							\item[]\ita{Average} approximately $O(n \log n)$
							\item[]\ita{Worst} already sorted, which leads to a
									hugely unbalanced split $(0, n-1)$. $T(n) =
									T(n-1) + T(0) + O(n) = O(n^2)$
					\end{itemize}
	\end{itemize}

	\subsubsection{Possible Improvements}
		\begin{itemize}
				\item[]\textbf{Switch to \texttt{INSERTION-SORT}}\par{note that
								in the worst-case scenario it is much faster
								$O(n)$. So when calling \texttt{QUICKSORT} on a
								subarray with fewer than $k$ elements, return
								without sorting, when the top-level call returns
								run \texttt{IS}
						}
				\item[]\textbf{Tail Call Optimisation}\par{only one recursive
								call ; hard to implement, but good compilers do
						it automatically}
				\item[]\textbf{Iterative with Stack}
				\item[]\textbf{3-Way \texttt{Quick-Sort}}\par{split the initial
								array into 3 subarrays, the new one including
						the elements equal to the pivot\mymarginpar{based on the
				dutch national flag algorithm}}
		\end{itemize}
