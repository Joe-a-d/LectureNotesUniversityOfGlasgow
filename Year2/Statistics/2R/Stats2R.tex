
%% CLASS MANUAL FOUND IN http://blog.poormansmath.net/latex-class-for-lecture-notes/ %%
%% CLASS AUTHOR Stefano Maggiolo %%
\documentclass[english,course]{Notes}

\title{Statistics 2R}
\subject{Statistics}
\author{Joao Almeida-Domingues}
\email{2334590D@student.gla.ac.uk}
\speaker{Alexey Lindo}
\date{23}{09}{2019}
\dateend{04}{12}{2019}
\place{University of Glasgow}

 %%%%% GENERAL MATHEMATICAL NOTATION SHORTCUTS %%%%%
 
\newcommand{\n}{\mathbb{N}}
\newcommand{\z}{\mathbb{Z}}
\newcommand{\q}{\mathbb{Q}}
\newcommand{\cx}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\ita}[1]{\textit{#1}}
\newcommand{\oneton}{\{1,2,3,...,n\}}
\newcommand\ef{\ita{f} }
\newcommand\comb[2]{^{#1}C_{#2}}
\newcommand\perm[2]{^{#1}P_{#2}}
\newcommand\inv[1]{#1^{-1}}
\newcommand\setb[1]{\{#1\}}
\newcommand\en{\ita{n }}
\renewcommand\qedsymbol{QED} %QED instead of square
\newcommand\handleft{\HandCuffLeft}
\newcommand\handright{\HandCuffRight}
\usepackage{setspace}
\onehalfspacing


%%%%%%%%%%%%%%%%PACKAGES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{lipsum}  

\usepackage{amsmath,amsthm,amssymb,graphicx,mathtools,tikz,pgfplots} %maths
\usepackage{hyperref,framed,color,fancybox,todo} %layout

\usepackage[backend=biber, style=reading]{biblatex} %bibliography
\bibliography{}

\renewcommand{\abstractname}{\vspace{3\baselineskip}} %hack to remove abstract
\usepackage{bbding} %dingbats  for signposting

% framed :  \begin{shaded,frame,snugshade or leftbar} \definecolor{shadecolor}{rgb}{XYZ} to change color
%fancybox: \shadowbox,ovalbox or doublebox
%\extra for Extra content layout box
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%CLASS SHORTCUTS%%%%
%\lecture{day}{month}{year} for margin note 
%\begin{theorem} sdfsdf\end{theorem}  --> \theorem
%\begin{proposition} dfsdfs\end{proposition} --> \prop
%\begin{lemma} dsfsd \end{lemma} --> \lem
%\begin{corollary} f ffew \end{corollary}
%\begin{definition} fwewef w \end{definition} --> \defn
%\begin{example} feww e\end{example} --> \ex
%\begin{exercise} wefwe \end{exercise}
%\begin{remark} wef we \end{remark} --> \rem
%\begin{fact} wefe \end{fact}
%\begin{problem} wef ew \end{problem}
%\begin{conjecture} ewfew \end{conjecture}
%\begin{claim} few w \end{claim}
%\begin{notation} fewf \end{notation} --> \nota
%\mymarginpar for scriptsize margin

\begin{document}

\begin{abstract}
\par{These lecture notes were collated by me from a mixture of sources , the two main sources being the lecture notes provided by the lecturer and the content presented in-lecture. All other referenced material (if used) can be found in the \ita{Bibliography} and \ita{References} sections.}
\par{The primary goal of these notes is to function as a succinct but comprehensive revision aid, hence if you came by them via a search engine , please note that they're not intended to be a reflection of the quality of the materials referenced or the content lectured.}
\par{Lastly, with regards to formatting, the pdf doc was typeset in \LaTeX , using a modified version of Stefano Maggiolo's \href{http://blog.poormansmath.net/latex-class-for-lecture-notes/}{\underline{\textcolor{blue}{class}}}}
\end{abstract}

\newpage

\section{Introduction \& Terminology}

\defn{Experiment}{ any procedure that can be infinitely repeated and has a well-defined set of possible outcomes}

\defn{Trial}{ a single performance of an experiment}

\defn{Outcome}{ information obtained from one trial}

\defn{Stochastic Experiment}{ an experiment which has more than one possible outcome, even when performed under identical conditions, where it is not known in advance which of the outcomes will occur when next performed. (a.k.a random experiment)}

\defn{Sample Space}{a set that contains all the possible outcomes of a random experiment}

\rem{usually denoted by $S$ ; it can be finite, countable or uncountable}

\defn{Countable Set}{ infinite set whose elements can be counted (e.g. natural numbers)}

\defn{Uncountable Set}{not countable (e.g. real numbers)}

\defn{Event}{collection of outcomes}

\rem{Any event $E$ is necessarily a subset of $S$}

\defn{Simple Event}{an event that consists of a single outcome, i.e. $|E| = 1$}

\defn{Compound Event}{ $|E| \geq 2$}

\subsection{Sets}

\par{This material has been covered at length at most other courses, hence it is crucial for higher level mathematics. See 2F for proofs on some of the results presented below}


\defn{Universal Set (S)}{set which includes all possible outcomes}
\defn{Empty Set ($\emptyset$)}{set which includes no elements}
\defn{Complement Set ($E^{c}$)}{set which includes everything which is not in $E$.}
\defn{Union ($E \cup F$)}{set which includes elements which are in at least one of $E$ or $F$. \ita{``E or F or both''}}
\defn{Intersection ($E \cap F$)}{set which includes elements which are both in $E$ \textbf{and} $F$}
\defn{Disjoint Sets}{when $E \cap F$ = $\emptyset$}
\defn{Subset ($E \subset F$)}{all elements of $E$ which are also in $F$; $E \implies F$}
\defn{Relative Complement Set ($F \setminus E$)}{set which includes all elements in $E$ except those which are also in E ; $F \setminus E = F \cap E^{c}$ }
\defn{Disjoint Union of Sets}{$F = (F \cap E) \cup (F \cap E^{c})$}

\textbf{Laws of Sets} \mymarginpar{All these results apply conversely to the intersection of events}
\begin{itemize}
\item[] Commutative : $A \cup B = B \cup A $
\item[] Associative : $ A \cup (B \cup C) = (A \cup B) \cup C$
\item[] Distributive : $ A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
\item[] DeMorgan's : $(A \cup B)^{c} = A^{c} \cap B^{c}$
\end{itemize}

\subsection{Interpreting Probability}\label{sec:1}

\defn{Probability of E ($P(E)$)}{ informs us on how likely $E$ is to occur in a given experiment}

\par{There are 3 main views, or ways to interpret the meaning of $P(E)$, nevertheless almost all of them use the same methods for calculating and manipulating it within a certain probability model. These will be covered in the next section}

\begin{enumerate}
\item \textbf{Equally-Likely Outcome (ELO)} : Assumes that for $k$ possible outcomes in $S$, all have the same likelihood of occurring, i.e. $$ P(E_{1}) = P(E_{2}) = \dots = P(E_{k}) = \frac{1}{k} =  $$

\item \textbf{Frequentist} : Assumes that for $k$ possible outcomes in $S$, the probability of a given event occurring is weighted by considering how many times it occurs in a large number of trials of a given experiment. The concepts of limit and \textbf{relative frequency} are useful here, for practical reasons.

$$ P(E) = \lim_{n \to \infty}\set{rf_{n}(E)} = \frac{n(E)}{n} $$ \mymarginpar{where $n(E)$ represents the number of outcomes in E}

\item \textbf{Subjectivist} : The probability of an event is best treated as a statement of personal belief.

\end{enumerate}

\newpage
\section{Counting ELOs}

\par{If $E$ is an event in a sample space, $S$, with $|S| = N$ equally likely (simple) outcomes , then $P(E)$ is the sum of the probabilities of the outcomes in E,}
$$ P(E) = \frac{\text{number of favourable outcomes}}{\text{number of total possible outcomes}} =  \frac{n(E)}{n(S)} = \frac{n(E)}{N}$$

\par{Note that the sizes of both $n(E)$ and $S$ can grow quite rapidly for relatively simple events, hence it is almost never desirable or feasible to exhaustively list them. Instead, we use combinatorics to count them.}

\subsection{Multiplication Principle}

\par{For a compound experiment $\lambda$ with component experiments $\lambda_{1} , \dots , \lambda_{n}$. If $\lambda_{1} $ has $k_{1}$ possible outcomes, then each of $k_{1}$ can pair with one of the $k_{2}$ possible outcomes of $\lambda_{2}$ and so on. So, no ``path'' needs to be necessarily taken, and therefore for each new experiment no new outcome needs to be excluded given the result of the previous. Hence, each event in $k_{2}$ could happen in any one of $k_{1}$ times, i.e the total possible number of outcomes for $\lambda$ is given my the \textbf{cartesian product} of its component's sample sets $$ |S| = \prod_{i=1}^{n}k_{i}$$}
\par{ Think of the way to create a one card as a compound experiment, where the first stage is picking the rank and the second is picking the suit. So our initial set is composed of 13 distinct possible outcomes. Our second set is composed of 4 distinct possible outcomes. Now note that choosing (1 out of the 4 possible suits) in the second stage can be achieved in 13 different ways, i.e our suit choice is not determined by the the rank chosen, each suit is available to each rank. Our  unique card is however determined by the combination of (R,S), so for each card I can choose 13 different ranks and I can do this 4 times for each different suit $(13C + 13D + 13H + 13S) = 13 \times 4  = |S1| \times |S2|$}

\subsection{Addition Principle}
\par{Unlike before, there are cases where certain outcomes can occur simultaneously. In this cases a choice in a component experiment will affect the number of possible outcomes available in the next experiment. Say for example if you can only score one 6 in a two-diced throw. Then, in the first stage you could either (A) get a 6 or (B) not get a 6. Note that if A, then the second stage cannot also involve A. If B, then the second stage cannot also involve B. So, we have $|S_{A}| = 1 \times 5$ which include all possible outcomes given that a 6 is scored in the first dice , and $|S_{B}| = 5 \times 1$ , which accounts for all cases where a 6 is scored in the second dice. In other words, for each stage we either exclude A or B given wether B or A happened before. $$|S| = \sum_{i=1}^{n} k_{n}$$}

\rem{Note that the number of distinct outcomes can still be the same, in which case $|S| = k \times n$}

\rem{In general, if we want to count the outcomes for $E_{1} \text{\textbf{ and }} B$ we use MP. $E_{1} \text{\textbf{ or }} B$ involves using AP}

\subsection{Counting using Combinatorics}

\par{In order to count the possible outcomes, we use the concept of Combinations and Permutations. Which gives us the total possible number of arrangements from a given set. The main difference between the two being the fact that one takes into consideration the order on which the elements are chosen, while the other is not.}

\subsubsection{Permutation}

\defn{Permutation}{total possible ordered arrangements from a set of distinct objects}

\notation{$\perm{n}{r}$}

$$\perm{n}{r} = \frac{n!}{(n-r)!}$$

\rem{Note from above that in the denominator we exclude all other distinct objects in the main set not chosen}

\rem{Note that if $n=r$, then $\perm{n}{r} = n!$}

\example{Say we need the total number of possible two distinct letter combination from the following set $\set{A,B,C,D}$. We could approach it as a multistage step problem, and use the multiplication principle in the following way: \\
\begin{enumerate}
\item From the $\set{A,B,C,D}$ choose 1 ; We know that there are 4 possible ways to do this. ; Say we choose A
\item Given that the objects need to be distinct we now have the set $\set{B,C,D}$ from which to choose, of which there are 3 possible ways to do so
\item Hence, we reach the conclusion that there are a total of $4 \times 3 = 12$ possible distinct arrangements
\end{enumerate}

\par{Note however that in a set of size 4 this is easy to see, but our initial goal was to find a method which works for large sets. Say, if you had 100 elements, and you needed a 50 element object then you would need $100 \times 99 \times 98 \dots \times 51$. Alternatively, we can attempt to generalize our method. }

\begin{enumerate}
\item We note that if we wanted to find the total number of arrangements for the original set, then that would just be $n!$ which is easy to compute with an aid of a calculator. 
\item Then we note that from that total number if we only need $r$ elements then there are $n-r$ elements for which we calculated possible arrangements which are not needed, and hence should not be accounted for in the total possible outcomes. In other words, there are $(n-r)!$ arrangements which we are not interested in
\item Finally, we exclude the arrangements in (2)  by dividing by (1). Giving us $$\frac{n!}{(n-r)!} = \perm{n}{r}$$
\end{enumerate}
}

\subsubsection{Combinations}

\defn{Combinations}{Subset of permutations, where the order does not matter. i.e $AB = BA$}

\par{Expanding on the notion of permutation above, all we need to do is to account for this repeated entries. We note that from the set of chosen objects, each object is composed of elements of size $r$, which can be arranged in $r!$ different ordered ways, i.e $\perm{r}{r}$. However, if the order doesn't matter, then this $r!$ count as 1 and need to be excluded.}

$$\comb{n}{r} = \frac{\perm{n}{r}}{\perm{r}{r}} = \frac{\frac{n!}{(n-r)!}}{\frac{r!}{(r-r)!}} = \frac{n!}{r!(n-r)!}$$

\rem{ The following useful results follow from above}
 $${ n \choose n} = {n \choose 0 } = 1 \qquad \text{ and } \qquad  {n \choose r }= {n \choose {n-r}}$$
 
 \section{Axioms of Probability}
 
 \par{As discussed in~\ref{sec:1} there is disagreement about the interpretation of $P(E)$ but there is general agreement about how valid deductions should be made within a probability model. There must be restrictions on the probabilities that can be assigned to events in S in order to make sure that the probabilities assigned to different events are consistent with each other. We say that a probability model is guaranteed to assign consistent probabilities to all events if it obeys the
following Axioms of Probability laid down by \ita{Kolmogorov (1933)}}
 
 \begin{itemize}
 \item[Axioms]
 \begin{enumerate}
 \item~\label{itm:ax1} $0 \leq P(E) \leq 1$ , for any event $E \subseteq S$ 
 \item~\label{itm:ax2} $P(S) = 1$ 
 \item~\label{itm:ax3}  $P(E_{1} \cup E_{2}) = P(E_{1}) + P(E_{2})$ if $E_{1} \cap E_{2} = \emptyset$ \mymarginpar{note that~\ref{itm:ax3} is a special case of~\ref{itm:ax4} where $E_{i>2} = \emptyset$}
 \item~\label{itm:ax4} If $\bigcup_{i=1}^{n}E_{i} = \emptyset$ , then $P(\bigcup_{i=1}^{n}E_{i}) = \sum_{i=1}^{n} P(E_{i})$ 
 \end{enumerate}
 \end{itemize}
 

 
\section{Rules of Probability}

\par{A common task within a given probability model is to calculate unknown probabilities from known ones. This could be calculated by successful application of the axioms above, however there are more key general results which can be derived from the axioms which make this process here. We call these results the \ita{rules of probability}}

\rem{The axioms always refer to \textbf{disjoint events} \mymarginpar{\handleft}  , hence this is a crucial fact to be aware of when deriving the  rules ; a common step when proving them is to split a compound event into its disjoint parts }
 
\prop{}{$P(E^{c}) = 1 - P(E)$~\label{rule1}}

   
\proofs{
\begin{align}
 P(E \cup E^{c}) = P(E) + P(E^{c}) \qquad (Axiom \ref{itm:ax3})\\
 P(E \cup E^{c}) = P(S) = 1 \qquad (Axiom \ref{itm:ax2}) \\
 P(E) + P(E^{c}) = 1 \iff P(E^{c}) = 1 - P(E) \qquad (1) , (2)
 \end{align}
}

\prop{\label{rule2}}{$P(\emptyset) = 0$}

\proofs{ We take $S^{c} = \emptyset$ then,  from~\ref{rule1} $ , P(\emptyset) = 1 - P(S) = 0$}

\prop{\label{rule3}}{$P(E) \leq P(F)$, for $E \subseteq F$}

\proofs{
\begin{align}
F &= E \cup F \setminus E \qquad  (\text{union of disjoint events}) \\
P(F) &= P(E) + P(F \setminus E) (Axiom \ref{itm:ax3}) \\
 &\geq P(E) , \text{ since } F \setminus E \geq 0  \qquad (Axiom \ref{itm:ax1}) 
\end{align}
}

\example{See Workshop1.6}

\prop{\label{rule4}}{$P(A \cup B) = P(A) + P(B) - P(A \cap B)$ \mymarginpar{\handright Generalization of \ref{itm:ax3} , where we take into account the fact that for non-disjoint events $A \cap B$ is counted twice}}

\proofs{
\begin{enumerate}
\item Split the compound event $A \cup B$ into  3 disjoint events
$$A \setminus B = X = A \cap B^{c} \qquad B \setminus A = Y = B \cap A^{c}  \qquad Z = A \cap B $$
$$P(A \cup B) = P(X) + P(Y) + P(Z)  \qquad (Axiom \ref{itm:ax4})$$

\item Now using Axiom \ref{itm:ax3}, we can start reconstructing the original sets from the disjoint ones  \mymarginpar{Look ahead, $A \cap B$ is present in the RHS of the equality, so probably does not need to be simplified}

$$ P(A) = P(X \cup Z) = P(X) + P(Z)  ; \qquad P(B) = P(Y \cup Z) = P(Y) + P(Z) $$ 

\item Finally, substituting the last line back into $P(A \cup B)$

\begin{align*}
P(A \cup B) &=  P(X) + P(Y) + P(Z) \\
&= \big(P(A) - P(Z)\big) + \big(P(B) - \cancel{P(Z)}\big) + \cancel{P(Z)} \\
&= P(A) + P(B) - P(Z) \\
&= P(A) + P(B) - P(A \cap B)
\end{align*}
\end{enumerate}

}

\prop{\label{rule5}}{$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$}

\proofs{Similar to above, but there are 7 disjoint events, hence writing out all the disjoint unions becomes slightly hairy, but the procedure is the same. See Workshop1.5 for full proof}

\prop{Boole's Inequality~\label{rule6}}{$$ P\Bigg(\bigcup_{i=1}^{n}E_{i}\Bigg) \leq \sum_{i=1}^{n} P\Bigg(E_{i}\Bigg)$$}

\proofs{By induction

\begin{enumerate}
\item \textbf{Base Case: } Show true for $n = 2$. From \ref{rule4} we have that,
\begin{align*}
L.H.S = P(E_{1} \cup E_{2}) &= P(E_{1}) + P(E_{2}) - P(E_{1} \cap E_{2})  \\
&\leq P(E_{1}) + P(E_{2}) = R.H.S \qquad  ( Axiom \ref{itm:ax1} )
\end{align*} \mymarginpar{Alternatively we can just note that $max(P(E_{1} \cup E_{2}))$ happens when they're disjoint}

\item \textbf{Induction Hypothesis:} Assume true for $n \geq 2$

\item \textbf{Induction Step:} Proving true for $n+1$
 $$P\Bigg(\bigcup_{i=1}^{n+1}E_{i}\Bigg) \leq \sum_{i=1}^{n+1} P\Bigg(E_{i}\Bigg) $$
\begin{align*}
L.H.S = P\Bigg(\bigcup_{i=1}^{n+1}E_{i}\Bigg) &= P\Bigg(\bigcup_{i=1}^{n}E_{i}\Bigg) \cup E_{n+1}) \\
&\leq P\Bigg(\bigcup_{i=1}^{n}E_{i}\Bigg) + P\Bigg(E_{n+1}\Bigg) \qquad from \ \ref{rule4} \\
&\leq \sum_{i=1}^{n} P\Bigg(E_{i}\Bigg) + P(E_{n+1}) \qquad by \ the \ induction  \ hypothesis \\
&\leq \sum_{i=1}^{n+1} P\Bigg(E_{i}\Bigg) = R.H.S
\end{align*} 

\end{enumerate}
}

\example{See Workshop 1.6 , 1.7}

\section{Random Variables}

\defn{Random Variable}{A function $X : S \to \real $ , which associates a unique numerical value $X(s)$ with every outcome $s \in S$}

\par{When working with random variables, we can summarise each outcome of an experiment with a single numerical value. In general, R.V can be split into two broad categories ,  depending on the values they can assume, they can either be (i) Discrete (ii) Continuous .}

\subsection{Discrete Random Variables}

\defn{DRV}{a random variable with countable range, i.e where $X(s)$ is countable for all $s$}

\defn{Realisation}{Each $X(s)$}

\defn{Range Space}{All possible $X(s)$}

\notation{$R_{x}$}

\example{
\textbf{Likert scale} , we have $S = \set{\text{``Strongly disagree``} , \dots , \text{``Strongly Agree''}}$ , and we can map each response into a numerical value such that $X(s) = \set{-2 , \dots , 2}$}

\par{Although  usually we speak of probabilities associated with $X$ , $P(X = 0) , P(X < 2)$ , these probabilities are induced from probabilities of equivalent events in the original sample space. For example $P(X = x)$ corresponds to the probability of the equivalent event $\set{s \in S : X(S) = x}$}

\defn{Probability Mass Function}{list of probabilities associated with each of its possible values.  $p_{x}(x) = P(X = x) , x \in \real$ , such that $0 \leq p_{x}(X) \leq 1 , x \in \real$ and $\sum_{x \in R_{x}} p_{x}(x) =1$}

\rem{$p_{x}(x) = 0 , \ \forall \ x \not\in R_{x}$}

\rem{It is common to represent p.m.f as the set of pairs $\set{(x , p_{x}(x)) , x \in R_{x}}$}

\defn{Cumulative Distribution Function}{function giving the probability that  $X \leq x$ : $ F_{x}(x) = P(X \leq x)$ , such that 

\begin{enumerate}
	\item $0 \leq F_{x}(X) \leq 1$ 
	\item $F_{x}(-\infty) = P(X \leq -\infty) = 0 \text{  and  } F_{x}(\infty) = P(X \leq \infty) = 1$
	\item $ x_{1} \leq x_{2}$ , then $P(X \leq x_{1}) \leq P(X \leq x_{2}) \equiv F_{x}(x_{1}) \leq F_{x}(x_{2})$ , i.e \textbf{increasing function}
\end{enumerate}
}

\rem{Given the discrete nature of the drv , then c.d.f for a given $x$ is just the sum of the p.m.f for each discrete value smaller than $x$ , i.e $F_{x}(x) = \sum_{r \in R_{x} \leq x} p_{x}(x) , x \in \real$} 

\par{Given the relation between p.m.f and c.d.f , we can always revert the relation and instead recover the first from the latter by subtracting consecutive terms : $p_{x}(1) = F_{x}(1) - F_{x}(0)$ , in general $$p_{x}(x) = F_{x}(x) - F_{x}(x-1)$$}

\todo{complete example 8.4 lecture notes ; p.45,56,48,49}
\section{Distributions}

\defn{Gamma Function}{ $$\Gamma(\alpha) = \int x^{\alpha - 1} e^{-x} dx = (\alpha - 1)! , \; \alpha = 1,2,\dots$$}

\subsection{Continuous Random Variables}

\par{For continuous variables it doesn't make sense to look at $P(X = x)$, instead we look at the probability for a given range of $x$}

\prop{}{$P(X = x) = 0 \; \forall x $}

\rem{For continuous distributions, $P(X=x)=0$, so the expressions $P(X \leq x) = P(X < x)$}


\todo{Computing prob from distr}


\subsubsection{Normal}

\extra{Computation}{Standard Normal}{
\begin{enumerate}
	\item Compute the mean $\mu = E(X)$ , and standard deviation $\sigma = \sqrt{Var(X)}$
	\item Input into formula $Z = \frac{X -  \mu}{\sigma}$
	\item Find appropriate $Z$ in cdf tables ; Recall for $x$ in table , we have $\phi(x) = Z  \leq x$ and for $-x$ we have $1-\phi(x)$ = $Z \leq -x $
\end{enumerate}
}



\example{ For $X\sim N(-1, 0.25)$ we have $Z = \frac{X - (-1)}{\sqrt{0.25}} \sim N(0,1)$. Hence,

\begin{enumerate}
\item  $\mathbf{P(X < 0.5)}$

$P(Z < \frac{0.5 - (-1)}{\sqrt{0.25}}) = P(Z < 3)$ , consulting the values of the $\mathop{c.d.f}$ we have $\phi(3) = P(X < 0.5) = 0.9987$

\item $\mathbf{P(X < -1.25)}$

$P(Z < \frac{-1.25 - (-1)}{\sqrt{0.25}}) = P(Z < -0.5)$ , consulting the values of the $\mathop{c.d.f}$ we have $1 - \phi(0.5) = P(X < -1.25) = 0.3805$

\item $\mathbf{P(-2 < X < 0)}$

$P(\frac{-2 - (-1)}{\sqrt{0.25}} < Z < \frac{0 - (-1)}{\sqrt{0.25}}) = P(-2 < Z < 2)$. Hence $Z > -2$ and $Z < 2$, which gives us the range $P(Z < 2) - P(Z < -2)$ consulting the values of the $\mathop{c.d.f}$ we have $\phi(2) = P(X < 2) = 0.9772$ and $ 1- \phi(2) = P(X < -2) $ therefore, $P(Z < 2) - P(Z < -2) = P(-2 < X < 0) = 2\phi(2) - 1 = 0.9544$ 

\item \textbf{Expected  value,  variance ,  standard deviation , median of X}

$E(X) = -1 \; ; \; \mathop{Var}(X) = 0.25 \; ; \; \sigma = 0.5 \; ; \; P(X < \varepsilon_{50}) = 0.5 \implies \varepsilon_{50} = -1 $


\end{enumerate}
}



\todo{Add to continuous subsec} 

\subsection{Gamma}




\newpage

\todo*{\url{https://en.wikipedia.org/wiki/Experiment_(probability_theory)}}
\todo*{\url{https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic09_7p2_Galvin.pdf}}
\todos




%\printbibliography

\end{document}